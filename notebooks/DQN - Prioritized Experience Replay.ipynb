{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The DQN improvement: Prioritized Experience Replay (based on https://arxiv.org/abs/1511.05952)\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "\n",
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "\n",
    "    Story the data with it priority in tree and data frameworks.\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add_new_priority(self, p, data):\n",
    "        leaf_idx = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(leaf_idx, p)  # update tree_frame\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "\n",
    "        self.tree[tree_idx] = p\n",
    "        self._propagate_change(tree_idx, change)\n",
    "\n",
    "    def _propagate_change(self, tree_idx, change):\n",
    "        \"\"\"change the sum of priority value in all parent nodes\"\"\"\n",
    "        parent_idx = (tree_idx - 1) // 2\n",
    "        self.tree[parent_idx] += change\n",
    "        if parent_idx != 0:\n",
    "            self._propagate_change(parent_idx, change)\n",
    "\n",
    "    def get_leaf(self, lower_bound):\n",
    "        leaf_idx = self._retrieve(lower_bound)  # search the max leaf priority based on the lower_bound\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return [leaf_idx, self.tree[leaf_idx], self.data[data_idx]]\n",
    "\n",
    "    def _retrieve(self, lower_bound, parent_idx=0):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        left_child_idx = 2 * parent_idx + 1\n",
    "        right_child_idx = left_child_idx + 1\n",
    "\n",
    "        if left_child_idx >= len(self.tree):  # end search when no more child\n",
    "            return parent_idx\n",
    "\n",
    "        if self.tree[left_child_idx] == self.tree[right_child_idx]:\n",
    "            return self._retrieve(lower_bound, np.random.choice([left_child_idx, right_child_idx]))\n",
    "        if lower_bound <= self.tree[left_child_idx]:  # downward search, always search for a higher priority node\n",
    "            return self._retrieve(lower_bound, left_child_idx)\n",
    "        else:\n",
    "            return self._retrieve(lower_bound - self.tree[left_child_idx], right_child_idx)\n",
    "\n",
    "    @property\n",
    "    def root_priority(self):\n",
    "        return self.tree[0]  # the root\n",
    "\n",
    "\n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.001  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 1e-4  # annealing the bias\n",
    "    abs_err_upper = 1   # for stability refer to paper\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, error, transition):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add_new_priority(p, transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch_idx, batch_memory, ISWeights = [], [], []\n",
    "        segment = self.tree.root_priority / n\n",
    "        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.root_priority\n",
    "        maxiwi = np.power(self.tree.capacity * min_prob, -self.beta)  # for later normalizing ISWeights\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            lower_bound = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(lower_bound)\n",
    "            prob = p / self.tree.root_priority\n",
    "            ISWeights.append(self.tree.capacity * prob)\n",
    "            batch_idx.append(idx)\n",
    "            batch_memory.append(data)\n",
    "\n",
    "        ISWeights = np.vstack(ISWeights)\n",
    "        ISWeights = np.power(ISWeights, -self.beta) / maxiwi  # normalize\n",
    "        return batch_idx, np.vstack(batch_memory), ISWeights\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        error += self.epsilon   # avoid 0\n",
    "        clipped_error = np.clip(error, 0, self.abs_err_upper)\n",
    "        return np.power(clipped_error, self.alpha)\n",
    "\n",
    "\n",
    "class DuelingDQNPrioritizedReplay:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.005,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=500,\n",
    "            memory_size=10000,\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            hidden=[100, 50],\n",
    "            output_graph=False,\n",
    "            sess=None,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden = hidden\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0.5 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        self.learn_step_counter = 0\n",
    "        self._build_net()\n",
    "        self.memory = Memory(capacity=memory_size)\n",
    "\n",
    "        if sess is None:\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            self.sess = sess\n",
    "\n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        def build_layers(s, c_names, w_initializer, b_initializer):\n",
    "            for i, h in enumerate(self.hidden):\n",
    "                if i == 0:\n",
    "                    in_units, out_units, inputs = self.n_features, self.hidden[i], s\n",
    "                else:\n",
    "                    in_units, out_units, inputs = self.hidden[i-1], self.hidden[i], l\n",
    "                with tf.variable_scope('l%i' % i):\n",
    "                    w = tf.get_variable('w', [in_units, out_units], initializer=w_initializer, collections=c_names)\n",
    "                    b = tf.get_variable('b', [1, out_units], initializer=b_initializer, collections=c_names)\n",
    "                    l = tf.nn.relu(tf.matmul(inputs, w) + b)\n",
    "\n",
    "            with tf.variable_scope('Value'):\n",
    "                w = tf.get_variable('w', [self.hidden[-1], 1], initializer=w_initializer, collections=c_names)\n",
    "                b = tf.get_variable('b', [1, 1], initializer=b_initializer, collections=c_names)\n",
    "                self.V = tf.matmul(l, w) + b\n",
    "\n",
    "            with tf.variable_scope('Advantage'):\n",
    "                w = tf.get_variable('w', [self.hidden[-1], self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b = tf.get_variable('b', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.A = tf.matmul(l, w) + b\n",
    "\n",
    "            with tf.variable_scope('Q'):\n",
    "                out = self.V + (self.A - tf.reduce_mean(self.A, axis=1, keep_dims=True))  # Q = V(s) + A(s,a)\n",
    "\n",
    "            # with tf.variable_scope('out'):\n",
    "            #     w = tf.get_variable('w', [self.hidden[-1], self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "            #     b = tf.get_variable('b', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "            #     out = tf.matmul(l, w) + b\n",
    "            return out\n",
    "\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss\n",
    "        self.ISWeights = tf.placeholder(tf.float32, [None, 1], name='IS_weights')\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            c_names, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], \\\n",
    "                tf.random_normal_initializer(0., 0.01), tf.constant_initializer(0.01)  # config of layers\n",
    "\n",
    "            self.q_eval = build_layers(self.s, c_names, w_initializer, b_initializer)\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.abs_errors = tf.abs(tf.reduce_sum(self.q_target - self.q_eval, axis=1))  # for updating Sumtree\n",
    "            self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.q_target, self.q_eval))\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')  # input\n",
    "        with tf.variable_scope('target_net'):\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            self.q_next = build_layers(self.s_, c_names, w_initializer, b_initializer)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        max_p = np.max(self.memory.tree.tree[-self.memory.tree.capacity:])\n",
    "        self.memory.store(max_p, transition)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        observation = observation[np.newaxis, :]\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def _replace_target_params(self):\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        self.sess.run([tf.assign(t, e) for t, e in zip(t_params, e_params)])\n",
    "\n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self._replace_target_params()\n",
    "\n",
    "        tree_idx, batch_memory, ISWeights = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # double DQN\n",
    "        q_next, q_eval4next = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={self.s_: batch_memory[:, -self.n_features:],  # next observation\n",
    "                       self.s: batch_memory[:, -self.n_features:]})  # next observation\n",
    "        q_eval = self.sess.run(self.q_eval, {self.s: batch_memory[:, :self.n_features]})\n",
    "\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "        max_act4next = np.argmax(q_eval4next,\n",
    "                                 axis=1)  # the action that brings the highest value is evaluated by q_eval\n",
    "        selected_q_next = q_next[batch_index, max_act4next]  # Double DQN, select q_next depending on above actions\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * selected_q_next\n",
    "\n",
    "\n",
    "        _, abs_errors, self.cost = self.sess.run([self._train_op, self.abs_errors, self.loss],\n",
    "                                                 feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                            self.q_target: q_target,\n",
    "                                                            self.ISWeights: ISWeights})\n",
    "        for i in range(len(tree_idx)):  # update priority\n",
    "            idx = tree_idx[i]\n",
    "            self.memory.update(idx, abs_errors[i])\n",
    "\n",
    "        self.cost_his.append(self.cost)\n",
    "\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epi:  0 | ------ | Epi_R:  -1.61 | Running_R:  -0.02 | Epsilon:  0.5\n",
      "Epi:  1 | ------ | Epi_R:  -4.99 | Running_R:  -0.07 | Epsilon:  0.5\n",
      "Epi:  2 | ------ | Epi_R:  -2.36 | Running_R:  -0.09 | Epsilon:  0.5\n",
      "Epi:  3 | ------ | Epi_R:  -5.23 | Running_R:  -0.14 | Epsilon:  0.5\n",
      "Epi:  4 | ------ | Epi_R:  -4.4 | Running_R:  -0.18 | Epsilon:  0.5\n",
      "Epi:  5 | ------ | Epi_R:  -4.25 | Running_R:  -0.22 | Epsilon:  0.5\n",
      "Epi:  6 | ------ | Epi_R:  -3.24 | Running_R:  -0.25 | Epsilon:  0.5\n",
      "Epi:  7 | ------ | Epi_R:  -3.91 | Running_R:  -0.29 | Epsilon:  0.5\n",
      "Epi:  8 | ------ | Epi_R:  -3.96 | Running_R:  -0.33 | Epsilon:  0.5\n",
      "Epi:  9 | ------ | Epi_R:  -3.81 | Running_R:  -0.36 | Epsilon:  0.5\n",
      "Epi:  10 | ------ | Epi_R:  -3.46 | Running_R:  -0.39 | Epsilon:  0.5\n",
      "Epi:  11 | ------ | Epi_R:  -2.23 | Running_R:  -0.41 | Epsilon:  0.5\n",
      "Epi:  12 | ------ | Epi_R:  -3.77 | Running_R:  -0.44 | Epsilon:  0.5\n",
      "Epi:  13 | ------ | Epi_R:  -3.85 | Running_R:  -0.48 | Epsilon:  0.5\n",
      "Epi:  14 | ------ | Epi_R:  -6.7 | Running_R:  -0.54 | Epsilon:  0.5\n",
      "Epi:  15 | ------ | Epi_R:  -5.21 | Running_R:  -0.59 | Epsilon:  0.5\n",
      "Epi:  16 | ------ | Epi_R:  -2.12 | Running_R:  -0.6 | Epsilon:  0.5\n",
      "Epi:  17 | ------ | Epi_R:  -4.18 | Running_R:  -0.64 | Epsilon:  0.5\n",
      "Epi:  18 | ------ | Epi_R:  -4.11 | Running_R:  -0.67 | Epsilon:  0.5\n",
      "Epi:  19 | ------ | Epi_R:  -5.61 | Running_R:  -0.72 | Epsilon:  0.5\n",
      "Epi:  20 | ------ | Epi_R:  -3.44 | Running_R:  -0.75 | Epsilon:  0.5\n",
      "Epi:  21 | ------ | Epi_R:  -4.67 | Running_R:  -0.79 | Epsilon:  0.5\n",
      "Epi:  22 | ------ | Epi_R:  -2.88 | Running_R:  -0.81 | Epsilon:  0.5\n",
      "Epi:  23 | ------ | Epi_R:  -2.87 | Running_R:  -0.83 | Epsilon:  0.5\n",
      "Epi:  24 | ------ | Epi_R:  -2.33 | Running_R:  -0.85 | Epsilon:  0.5\n",
      "Epi:  25 | ------ | Epi_R:  -5.09 | Running_R:  -0.89 | Epsilon:  0.5\n",
      "Epi:  26 | ------ | Epi_R:  -2.65 | Running_R:  -0.91 | Epsilon:  0.5\n",
      "Epi:  27 | ------ | Epi_R:  -8.15 | Running_R:  -0.98 | Epsilon:  0.5\n",
      "Epi:  28 | ------ | Epi_R:  -4.19 | Running_R:  -1.01 | Epsilon:  0.5\n",
      "Epi:  29 | ------ | Epi_R:  -4.56 | Running_R:  -1.05 | Epsilon:  0.5\n",
      "Epi:  30 | ------ | Epi_R:  -2.87 | Running_R:  -1.06 | Epsilon:  0.5\n",
      "Epi:  31 | ------ | Epi_R:  -4.1 | Running_R:  -1.09 | Epsilon:  0.5\n",
      "Epi:  32 | ------ | Epi_R:  -2.6 | Running_R:  -1.11 | Epsilon:  0.5\n",
      "Epi:  33 | ------ | Epi_R:  -4.46 | Running_R:  -1.14 | Epsilon:  0.5\n",
      "Epi:  34 | ------ | Epi_R:  -1.91 | Running_R:  -1.15 | Epsilon:  0.5\n",
      "Epi:  35 | ------ | Epi_R:  -3.27 | Running_R:  -1.17 | Epsilon:  0.5\n",
      "Epi:  36 | ------ | Epi_R:  -5.4 | Running_R:  -1.21 | Epsilon:  0.5\n",
      "Epi:  37 | ------ | Epi_R:  -7.26 | Running_R:  -1.27 | Epsilon:  0.5\n",
      "Epi:  38 | ------ | Epi_R:  -2.09 | Running_R:  -1.28 | Epsilon:  0.5\n",
      "Epi:  39 | ------ | Epi_R:  -1.32 | Running_R:  -1.28 | Epsilon:  0.5\n",
      "Epi:  40 | ------ | Epi_R:  -2.01 | Running_R:  -1.29 | Epsilon:  0.5\n",
      "Epi:  41 | ------ | Epi_R:  -2.05 | Running_R:  -1.3 | Epsilon:  0.5\n",
      "Epi:  42 | ------ | Epi_R:  -4.54 | Running_R:  -1.33 | Epsilon:  0.5\n",
      "Epi:  43 | ------ | Epi_R:  -3.47 | Running_R:  -1.35 | Epsilon:  0.5\n",
      "Epi:  44 | ------ | Epi_R:  -2.77 | Running_R:  -1.37 | Epsilon:  0.5\n",
      "Epi:  45 | ------ | Epi_R:  -3.44 | Running_R:  -1.39 | Epsilon:  0.5\n",
      "Epi:  46 | ------ | Epi_R:  -3.18 | Running_R:  -1.4 | Epsilon:  0.5\n",
      "Epi:  47 | ------ | Epi_R:  -4.96 | Running_R:  -1.44 | Epsilon:  0.5\n",
      "Epi:  48 | ------ | Epi_R:  -7.79 | Running_R:  -1.5 | Epsilon:  0.5\n",
      "Epi:  49 | ------ | Epi_R:  -2.22 | Running_R:  -1.51 | Epsilon:  0.5\n",
      "Epi:  50 | ------ | Epi_R:  -4.91 | Running_R:  -1.54 | Epsilon:  0.5\n",
      "Epi:  51 | ------ | Epi_R:  -3.95 | Running_R:  -1.57 | Epsilon:  0.5\n",
      "Epi:  52 | ------ | Epi_R:  -3.38 | Running_R:  -1.59 | Epsilon:  0.5\n",
      "Epi:  53 | ------ | Epi_R:  -4.24 | Running_R:  -1.61 | Epsilon:  0.5\n",
      "Epi:  54 | ------ | Epi_R:  -2.56 | Running_R:  -1.62 | Epsilon:  0.5\n",
      "Epi:  55 | ------ | Epi_R:  -6.34 | Running_R:  -1.67 | Epsilon:  0.5\n",
      "Epi:  56 | ------ | Epi_R:  -4.17 | Running_R:  -1.69 | Epsilon:  0.5\n",
      "Epi:  57 | ------ | Epi_R:  -2.26 | Running_R:  -1.7 | Epsilon:  0.5\n",
      "Epi:  58 | ------ | Epi_R:  -2.71 | Running_R:  -1.71 | Epsilon:  0.5\n",
      "Epi:  59 | ------ | Epi_R:  -3.77 | Running_R:  -1.73 | Epsilon:  0.5\n",
      "Epi:  60 | ------ | Epi_R:  -1.21 | Running_R:  -1.73 | Epsilon:  0.5\n",
      "Epi:  61 | ------ | Epi_R:  -4.72 | Running_R:  -1.76 | Epsilon:  0.5\n",
      "Epi:  62 | ------ | Epi_R:  -0.42 | Running_R:  -1.74 | Epsilon:  0.5\n",
      "Epi:  63 | ------ | Epi_R:  -3.28 | Running_R:  -1.76 | Epsilon:  0.5\n",
      "Epi:  64 | ------ | Epi_R:  -2.92 | Running_R:  -1.77 | Epsilon:  0.5\n",
      "Epi:  65 | ------ | Epi_R:  -8.91 | Running_R:  -1.84 | Epsilon:  0.5\n",
      "Epi:  66 | ------ | Epi_R:  -3.83 | Running_R:  -1.86 | Epsilon:  0.5\n",
      "Epi:  67 | ------ | Epi_R:  -1.94 | Running_R:  -1.86 | Epsilon:  0.5\n",
      "Epi:  68 | ------ | Epi_R:  -3.16 | Running_R:  -1.87 | Epsilon:  0.5\n",
      "Epi:  69 | ------ | Epi_R:  -3.5 | Running_R:  -1.89 | Epsilon:  0.5\n",
      "Epi:  70 | ------ | Epi_R:  -3.45 | Running_R:  -1.91 | Epsilon:  0.5\n",
      "Epi:  71 | ------ | Epi_R:  -3.31 | Running_R:  -1.92 | Epsilon:  0.5\n",
      "Epi:  72 | ------ | Epi_R:  -3.84 | Running_R:  -1.94 | Epsilon:  0.5\n",
      "Epi:  73 | ------ | Epi_R:  -2.02 | Running_R:  -1.94 | Epsilon:  0.5\n",
      "Epi:  74 | ------ | Epi_R:  -3.21 | Running_R:  -1.95 | Epsilon:  0.5\n",
      "Epi:  75 | ------ | Epi_R:  -3.97 | Running_R:  -1.97 | Epsilon:  0.5\n",
      "Epi:  76 | ------ | Epi_R:  -2.41 | Running_R:  -1.98 | Epsilon:  0.5\n",
      "Epi:  77 | ------ | Epi_R:  -1.54 | Running_R:  -1.97 | Epsilon:  0.5\n",
      "Epi:  78 | ------ | Epi_R:  -10.5 | Running_R:  -2.06 | Epsilon:  0.5\n",
      "Epi:  79 | ------ | Epi_R:  -3.63 | Running_R:  -2.07 | Epsilon:  0.5\n",
      "Epi:  80 | ------ | Epi_R:  -4.09 | Running_R:  -2.09 | Epsilon:  0.5\n",
      "Epi:  81 | ------ | Epi_R:  -5.87 | Running_R:  -2.13 | Epsilon:  0.5\n",
      "Epi:  82 | ------ | Epi_R:  -4.82 | Running_R:  -2.16 | Epsilon:  0.5\n",
      "Epi:  83 | ------ | Epi_R:  -3.73 | Running_R:  -2.18 | Epsilon:  0.5\n",
      "Epi:  84 | ------ | Epi_R:  -3.15 | Running_R:  -2.18 | Epsilon:  0.5\n",
      "Epi:  85 | ------ | Epi_R:  -4.2 | Running_R:  -2.21 | Epsilon:  0.5\n",
      "Epi:  86 | ------ | Epi_R:  -3.46 | Running_R:  -2.22 | Epsilon:  0.5\n",
      "Epi:  87 | ------ | Epi_R:  -3.65 | Running_R:  -2.23 | Epsilon:  0.5\n",
      "Epi:  88 | ------ | Epi_R:  -2.15 | Running_R:  -2.23 | Epsilon:  0.5\n",
      "Epi:  89 | ------ | Epi_R:  -3.4 | Running_R:  -2.24 | Epsilon:  0.5\n",
      "Epi:  90 | ------ | Epi_R:  -2.93 | Running_R:  -2.25 | Epsilon:  0.5\n",
      "Epi:  91 | ------ | Epi_R:  -6.2 | Running_R:  -2.29 | Epsilon:  0.5\n",
      "Epi:  92 | ------ | Epi_R:  -4.67 | Running_R:  -2.31 | Epsilon:  0.5\n",
      "Epi:  93 | ------ | Epi_R:  -3.72 | Running_R:  -2.33 | Epsilon:  0.5\n",
      "Epi:  94 | ------ | Epi_R:  -3.14 | Running_R:  -2.34 | Epsilon:  0.5\n",
      "Epi:  95 | ------ | Epi_R:  -4.98 | Running_R:  -2.36 | Epsilon:  0.5\n",
      "Epi:  96 | ------ | Epi_R:  -4.04 | Running_R:  -2.38 | Epsilon:  0.5\n",
      "Epi:  97 | ------ | Epi_R:  -3.08 | Running_R:  -2.39 | Epsilon:  0.5\n",
      "Epi:  98 | ------ | Epi_R:  0.39 | Running_R:  -2.36 | Epsilon:  0.5\n",
      "Epi:  99 | ------ | Epi_R:  -1.55 | Running_R:  -2.35 | Epsilon:  0.5\n",
      "Epi:  100 | ------ | Epi_R:  -2.0 | Running_R:  -2.35 | Epsilon:  0.5\n",
      "Epi:  101 | ------ | Epi_R:  -1.98 | Running_R:  -2.34 | Epsilon:  0.5\n",
      "Epi:  102 | ------ | Epi_R:  -3.94 | Running_R:  -2.36 | Epsilon:  0.5\n",
      "Epi:  103 | ------ | Epi_R:  -2.91 | Running_R:  -2.36 | Epsilon:  0.5\n",
      "Epi:  104 | ------ | Epi_R:  -2.44 | Running_R:  -2.36 | Epsilon:  0.5\n",
      "Epi:  105 | ------ | Epi_R:  -2.46 | Running_R:  -2.37 | Epsilon:  0.5\n",
      "Epi:  106 | ------ | Epi_R:  -2.46 | Running_R:  -2.37 | Epsilon:  0.5\n",
      "Epi:  107 | ------ | Epi_R:  -2.57 | Running_R:  -2.37 | Epsilon:  0.5\n",
      "Epi:  108 | ------ | Epi_R:  -2.12 | Running_R:  -2.37 | Epsilon:  0.5\n",
      "Epi:  109 | ------ | Epi_R:  -3.24 | Running_R:  -2.37 | Epsilon:  0.5\n",
      "Epi:  110 | ------ | Epi_R:  -3.7 | Running_R:  -2.39 | Epsilon:  0.5\n",
      "Epi:  111 | ------ | Epi_R:  -2.49 | Running_R:  -2.39 | Epsilon:  0.5\n",
      "Epi:  112 | ------ | Epi_R:  -2.2 | Running_R:  -2.39 | Epsilon:  0.5\n",
      "Epi:  113 | ------ | Epi_R:  -2.75 | Running_R:  -2.39 | Epsilon:  0.5\n",
      "Epi:  114 | ------ | Epi_R:  -3.91 | Running_R:  -2.41 | Epsilon:  0.5\n",
      "Epi:  115 | ------ | Epi_R:  -3.76 | Running_R:  -2.42 | Epsilon:  0.5\n",
      "Epi:  116 | ------ | Epi_R:  -2.13 | Running_R:  -2.42 | Epsilon:  0.5\n",
      "Epi:  117 | ------ | Epi_R:  -3.03 | Running_R:  -2.42 | Epsilon:  0.5\n",
      "Epi:  118 | ------ | Epi_R:  -4.24 | Running_R:  -2.44 | Epsilon:  0.5\n",
      "Epi:  119 | ------ | Epi_R:  -3.67 | Running_R:  -2.45 | Epsilon:  0.5\n",
      "Epi:  120 | ------ | Epi_R:  -1.72 | Running_R:  -2.45 | Epsilon:  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epi:  121 | ------ | Epi_R:  -3.11 | Running_R:  -2.45 | Epsilon:  0.5\n",
      "Epi:  122 | ------ | Epi_R:  -4.65 | Running_R:  -2.47 | Epsilon:  0.5\n",
      "Epi:  123 | ------ | Epi_R:  -4.13 | Running_R:  -2.49 | Epsilon:  0.5\n",
      "Epi:  124 | ------ | Epi_R:  -5.44 | Running_R:  -2.52 | Epsilon:  0.5\n",
      "Epi:  125 | ------ | Epi_R:  -3.0 | Running_R:  -2.53 | Epsilon:  0.5\n",
      "Epi:  126 | ------ | Epi_R:  -3.02 | Running_R:  -2.53 | Epsilon:  0.5\n",
      "Epi:  127 | ------ | Epi_R:  -6.56 | Running_R:  -2.57 | Epsilon:  0.5\n",
      "Epi:  128 | ------ | Epi_R:  -0.18 | Running_R:  -2.55 | Epsilon:  0.5\n",
      "Epi:  129 | ------ | Epi_R:  -2.89 | Running_R:  -2.55 | Epsilon:  0.5\n",
      "Epi:  130 | ------ | Epi_R:  -2.24 | Running_R:  -2.55 | Epsilon:  0.5\n",
      "Epi:  131 | ------ | Epi_R:  -3.25 | Running_R:  -2.55 | Epsilon:  0.5\n",
      "Epi:  132 | ------ | Epi_R:  -2.93 | Running_R:  -2.56 | Epsilon:  0.5\n",
      "Epi:  133 | ------ | Epi_R:  -1.16 | Running_R:  -2.54 | Epsilon:  0.5\n",
      "Epi:  134 | ------ | Epi_R:  -4.57 | Running_R:  -2.56 | Epsilon:  0.5\n",
      "Epi:  135 | ------ | Epi_R:  -1.73 | Running_R:  -2.56 | Epsilon:  0.5\n",
      "Epi:  136 | ------ | Epi_R:  -2.03 | Running_R:  -2.55 | Epsilon:  0.5\n",
      "Epi:  137 | ------ | Epi_R:  -4.37 | Running_R:  -2.57 | Epsilon:  0.5\n",
      "Epi:  138 | ------ | Epi_R:  -4.59 | Running_R:  -2.59 | Epsilon:  0.5\n",
      "Epi:  139 | ------ | Epi_R:  -3.35 | Running_R:  -2.6 | Epsilon:  0.5\n",
      "Epi:  140 | ------ | Epi_R:  -4.83 | Running_R:  -2.62 | Epsilon:  0.5\n",
      "Epi:  141 | ------ | Epi_R:  -1.81 | Running_R:  -2.61 | Epsilon:  0.5\n",
      "Epi:  142 | ------ | Epi_R:  -3.05 | Running_R:  -2.62 | Epsilon:  0.5\n",
      "Epi:  143 | ------ | Epi_R:  -4.01 | Running_R:  -2.63 | Epsilon:  0.5\n",
      "Epi:  144 | ------ | Epi_R:  -3.45 | Running_R:  -2.64 | Epsilon:  0.5\n",
      "Epi:  145 | ------ | Epi_R:  -4.5 | Running_R:  -2.66 | Epsilon:  0.5\n",
      "Epi:  146 | ------ | Epi_R:  -2.91 | Running_R:  -2.66 | Epsilon:  0.5\n",
      "Epi:  147 | ------ | Epi_R:  -3.49 | Running_R:  -2.67 | Epsilon:  0.5\n",
      "Epi:  148 | ------ | Epi_R:  -3.56 | Running_R:  -2.68 | Epsilon:  0.5\n",
      "Epi:  149 | ------ | Epi_R:  -2.44 | Running_R:  -2.67 | Epsilon:  0.5\n",
      "Epi:  150 | ------ | Epi_R:  -5.15 | Running_R:  -2.7 | Epsilon:  0.5\n",
      "Epi:  151 | ------ | Epi_R:  -3.58 | Running_R:  -2.71 | Epsilon:  0.5\n",
      "Epi:  152 | ------ | Epi_R:  -3.96 | Running_R:  -2.72 | Epsilon:  0.5\n",
      "Epi:  153 | ------ | Epi_R:  -5.03 | Running_R:  -2.74 | Epsilon:  0.5\n",
      "Epi:  154 | ------ | Epi_R:  -3.15 | Running_R:  -2.75 | Epsilon:  0.5\n",
      "Epi:  155 | ------ | Epi_R:  -4.91 | Running_R:  -2.77 | Epsilon:  0.5\n",
      "Epi:  156 | ------ | Epi_R:  -4.02 | Running_R:  -2.78 | Epsilon:  0.5\n",
      "Epi:  157 | ------ | Epi_R:  -4.25 | Running_R:  -2.8 | Epsilon:  0.5\n",
      "Epi:  158 | ------ | Epi_R:  -3.08 | Running_R:  -2.8 | Epsilon:  0.5\n",
      "Epi:  159 | ------ | Epi_R:  -6.78 | Running_R:  -2.84 | Epsilon:  0.5\n",
      "Epi:  160 | ------ | Epi_R:  -1.66 | Running_R:  -2.83 | Epsilon:  0.5\n",
      "Epi:  161 | ------ | Epi_R:  -2.28 | Running_R:  -2.82 | Epsilon:  0.5\n",
      "Epi:  162 | ------ | Epi_R:  -5.05 | Running_R:  -2.84 | Epsilon:  0.5\n",
      "Epi:  163 | ------ | Epi_R:  -0.17 | Running_R:  -2.82 | Epsilon:  0.5\n",
      "Epi:  164 | ------ | Epi_R:  -2.48 | Running_R:  -2.81 | Epsilon:  0.5\n",
      "Epi:  165 | ------ | Epi_R:  -5.61 | Running_R:  -2.84 | Epsilon:  0.5\n",
      "Epi:  166 | ------ | Epi_R:  -3.66 | Running_R:  -2.85 | Epsilon:  0.5\n",
      "Epi:  167 | ------ | Epi_R:  0.1 | Running_R:  -2.82 | Epsilon:  0.5\n",
      "Epi:  168 | ------ | Epi_R:  -3.58 | Running_R:  -2.83 | Epsilon:  0.5\n",
      "Epi:  169 | ------ | Epi_R:  -1.63 | Running_R:  -2.82 | Epsilon:  0.5\n",
      "Epi:  170 | ------ | Epi_R:  -5.95 | Running_R:  -2.85 | Epsilon:  0.5\n",
      "Epi:  171 | ------ | Epi_R:  -0.42 | Running_R:  -2.82 | Epsilon:  0.5\n",
      "Epi:  172 | ------ | Epi_R:  -3.99 | Running_R:  -2.83 | Epsilon:  0.5\n",
      "Epi:  173 | ------ | Epi_R:  -3.66 | Running_R:  -2.84 | Epsilon:  0.5\n",
      "Epi:  174 | ------ | Epi_R:  -3.03 | Running_R:  -2.84 | Epsilon:  0.5\n",
      "Epi:  175 | ------ | Epi_R:  -3.01 | Running_R:  -2.85 | Epsilon:  0.5\n",
      "Epi:  176 | ------ | Epi_R:  -2.77 | Running_R:  -2.85 | Epsilon:  0.5\n",
      "Epi:  177 | ------ | Epi_R:  -1.78 | Running_R:  -2.83 | Epsilon:  0.5\n",
      "Epi:  178 | ------ | Epi_R:  -3.14 | Running_R:  -2.84 | Epsilon:  0.5\n",
      "Epi:  179 | ------ | Epi_R:  -1.42 | Running_R:  -2.82 | Epsilon:  0.5\n",
      "Epi:  180 | ------ | Epi_R:  -8.13 | Running_R:  -2.88 | Epsilon:  0.5\n",
      "Epi:  181 | ------ | Epi_R:  -5.93 | Running_R:  -2.91 | Epsilon:  0.5\n",
      "Epi:  182 | ------ | Epi_R:  -3.0 | Running_R:  -2.91 | Epsilon:  0.5\n",
      "Epi:  183 | ------ | Epi_R:  -3.05 | Running_R:  -2.91 | Epsilon:  0.5\n",
      "Epi:  184 | ------ | Epi_R:  -4.65 | Running_R:  -2.93 | Epsilon:  0.5\n",
      "Epi:  185 | ------ | Epi_R:  -3.6 | Running_R:  -2.93 | Epsilon:  0.5\n",
      "Epi:  186 | ------ | Epi_R:  -2.55 | Running_R:  -2.93 | Epsilon:  0.5\n",
      "Epi:  187 | ------ | Epi_R:  0.48 | Running_R:  -2.9 | Epsilon:  0.5\n",
      "Epi:  188 | ------ | Epi_R:  -3.11 | Running_R:  -2.9 | Epsilon:  0.5\n",
      "Epi:  189 | ------ | Epi_R:  -3.26 | Running_R:  -2.9 | Epsilon:  0.5\n",
      "Epi:  190 | ------ | Epi_R:  -3.7 | Running_R:  -2.91 | Epsilon:  0.5\n",
      "Epi:  191 | ------ | Epi_R:  -3.71 | Running_R:  -2.92 | Epsilon:  0.5\n",
      "Epi:  192 | ------ | Epi_R:  -4.85 | Running_R:  -2.94 | Epsilon:  0.5\n",
      "Epi:  193 | ------ | Epi_R:  -4.78 | Running_R:  -2.96 | Epsilon:  0.5\n",
      "Epi:  194 | ------ | Epi_R:  -5.62 | Running_R:  -2.98 | Epsilon:  0.5\n",
      "Epi:  195 | ------ | Epi_R:  -3.34 | Running_R:  -2.99 | Epsilon:  0.5\n",
      "Epi:  196 | ------ | Epi_R:  -3.93 | Running_R:  -2.99 | Epsilon:  0.5\n",
      "Epi:  197 | ------ | Epi_R:  -4.32 | Running_R:  -3.01 | Epsilon:  0.5\n",
      "Epi:  198 | ------ | Epi_R:  -3.26 | Running_R:  -3.01 | Epsilon:  0.5\n",
      "Epi:  199 | ------ | Epi_R:  -0.6 | Running_R:  -2.99 | Epsilon:  0.5\n",
      "Epi:  200 | ------ | Epi_R:  -2.32 | Running_R:  -2.98 | Epsilon:  0.5\n",
      "Epi:  201 | ------ | Epi_R:  -4.87 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  202 | ------ | Epi_R:  -2.88 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  203 | ------ | Epi_R:  -0.98 | Running_R:  -2.98 | Epsilon:  0.5\n",
      "Epi:  204 | ------ | Epi_R:  -3.34 | Running_R:  -2.98 | Epsilon:  0.5\n",
      "Epi:  205 | ------ | Epi_R:  -4.49 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  206 | ------ | Epi_R:  -3.0 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  207 | ------ | Epi_R:  -4.15 | Running_R:  -3.01 | Epsilon:  0.5\n",
      "Epi:  208 | ------ | Epi_R:  -4.83 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  209 | ------ | Epi_R:  -1.91 | Running_R:  -3.01 | Epsilon:  0.5\n",
      "Epi:  210 | ------ | Epi_R:  -4.06 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  211 | ------ | Epi_R:  -1.38 | Running_R:  -3.01 | Epsilon:  0.5\n",
      "Epi:  212 | ------ | Epi_R:  -2.0 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  213 | ------ | Epi_R:  -3.19 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  214 | ------ | Epi_R:  -3.65 | Running_R:  -3.01 | Epsilon:  0.5\n",
      "Epi:  215 | ------ | Epi_R:  -2.64 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  216 | ------ | Epi_R:  -3.15 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  217 | ------ | Epi_R:  -3.86 | Running_R:  -3.01 | Epsilon:  0.5\n",
      "Epi:  218 | ------ | Epi_R:  -3.31 | Running_R:  -3.02 | Epsilon:  0.5\n",
      "Epi:  219 | ------ | Epi_R:  -1.86 | Running_R:  -3.0 | Epsilon:  0.5\n",
      "Epi:  220 | ------ | Epi_R:  -4.64 | Running_R:  -3.02 | Epsilon:  0.5\n",
      "Epi:  221 | ------ | Epi_R:  -4.82 | Running_R:  -3.04 | Epsilon:  0.5\n",
      "Epi:  222 | ------ | Epi_R:  -1.28 | Running_R:  -3.02 | Epsilon:  0.5\n",
      "Epi:  223 | ------ | Epi_R:  -2.77 | Running_R:  -3.02 | Epsilon:  0.5\n",
      "Epi:  224 | ------ | Epi_R:  -5.76 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  225 | ------ | Epi_R:  -2.35 | Running_R:  -3.04 | Epsilon:  0.5\n",
      "Epi:  226 | ------ | Epi_R:  -2.46 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  227 | ------ | Epi_R:  -2.37 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  228 | ------ | Epi_R:  -3.1 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  229 | ------ | Epi_R:  -4.03 | Running_R:  -3.04 | Epsilon:  0.5\n",
      "Epi:  230 | ------ | Epi_R:  -4.92 | Running_R:  -3.06 | Epsilon:  0.5\n",
      "Epi:  231 | ------ | Epi_R:  -3.55 | Running_R:  -3.06 | Epsilon:  0.5\n",
      "Epi:  232 | ------ | Epi_R:  0.07 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  233 | ------ | Epi_R:  -2.34 | Running_R:  -3.02 | Epsilon:  0.5\n",
      "Epi:  234 | ------ | Epi_R:  -3.75 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  235 | ------ | Epi_R:  -5.16 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  236 | ------ | Epi_R:  -1.01 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  237 | ------ | Epi_R:  -2.19 | Running_R:  -3.02 | Epsilon:  0.5\n",
      "Epi:  238 | ------ | Epi_R:  -3.62 | Running_R:  -3.03 | Epsilon:  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epi:  239 | ------ | Epi_R:  -3.61 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  240 | ------ | Epi_R:  -4.79 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  241 | ------ | Epi_R:  -3.01 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  242 | ------ | Epi_R:  -3.14 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  243 | ------ | Epi_R:  -2.64 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  244 | ------ | Epi_R:  -5.24 | Running_R:  -3.07 | Epsilon:  0.5\n",
      "Epi:  245 | ------ | Epi_R:  -1.82 | Running_R:  -3.06 | Epsilon:  0.5\n",
      "Epi:  246 | ------ | Epi_R:  -4.65 | Running_R:  -3.07 | Epsilon:  0.5\n",
      "Epi:  247 | ------ | Epi_R:  -3.48 | Running_R:  -3.08 | Epsilon:  0.5\n",
      "Epi:  248 | ------ | Epi_R:  -4.78 | Running_R:  -3.1 | Epsilon:  0.5\n",
      "Epi:  249 | ------ | Epi_R:  -2.75 | Running_R:  -3.09 | Epsilon:  0.5\n",
      "Epi:  250 | ------ | Epi_R:  -3.61 | Running_R:  -3.1 | Epsilon:  0.5\n",
      "Epi:  251 | ------ | Epi_R:  -5.59 | Running_R:  -3.12 | Epsilon:  0.5\n",
      "Epi:  252 | ------ | Epi_R:  -1.31 | Running_R:  -3.1 | Epsilon:  0.5\n",
      "Epi:  253 | ------ | Epi_R:  -2.92 | Running_R:  -3.1 | Epsilon:  0.5\n",
      "Epi:  254 | ------ | Epi_R:  -3.3 | Running_R:  -3.1 | Epsilon:  0.5\n",
      "Epi:  255 | ------ | Epi_R:  -2.56 | Running_R:  -3.1 | Epsilon:  0.5\n",
      "Epi:  256 | ------ | Epi_R:  -4.21 | Running_R:  -3.11 | Epsilon:  0.5\n",
      "Epi:  257 | ------ | Epi_R:  -4.25 | Running_R:  -3.12 | Epsilon:  0.5\n",
      "Epi:  258 | ------ | Epi_R:  -5.86 | Running_R:  -3.15 | Epsilon:  0.5\n",
      "Epi:  259 | ------ | Epi_R:  -3.06 | Running_R:  -3.15 | Epsilon:  0.5\n",
      "Epi:  260 | ------ | Epi_R:  -2.47 | Running_R:  -3.14 | Epsilon:  0.5\n",
      "Epi:  261 | ------ | Epi_R:  -3.05 | Running_R:  -3.14 | Epsilon:  0.5\n",
      "Epi:  262 | ------ | Epi_R:  -2.77 | Running_R:  -3.14 | Epsilon:  0.5\n",
      "Epi:  263 | ------ | Epi_R:  -4.31 | Running_R:  -3.15 | Epsilon:  0.5\n",
      "Epi:  264 | ------ | Epi_R:  -3.57 | Running_R:  -3.15 | Epsilon:  0.5\n",
      "Epi:  265 | ------ | Epi_R:  -2.53 | Running_R:  -3.15 | Epsilon:  0.5\n",
      "Epi:  266 | ------ | Epi_R:  -2.69 | Running_R:  -3.14 | Epsilon:  0.5\n",
      "Epi:  267 | ------ | Epi_R:  -1.0 | Running_R:  -3.12 | Epsilon:  0.5\n",
      "Epi:  268 | ------ | Epi_R:  -3.01 | Running_R:  -3.12 | Epsilon:  0.5\n",
      "Epi:  269 | ------ | Epi_R:  -4.64 | Running_R:  -3.13 | Epsilon:  0.5\n",
      "Epi:  270 | ------ | Epi_R:  -2.56 | Running_R:  -3.13 | Epsilon:  0.5\n",
      "Epi:  271 | ------ | Epi_R:  -1.69 | Running_R:  -3.11 | Epsilon:  0.5\n",
      "Epi:  272 | ------ | Epi_R:  -0.36 | Running_R:  -3.09 | Epsilon:  0.5\n",
      "Epi:  273 | ------ | Epi_R:  -5.42 | Running_R:  -3.11 | Epsilon:  0.5\n",
      "Epi:  274 | ------ | Epi_R:  -1.39 | Running_R:  -3.09 | Epsilon:  0.5\n",
      "Epi:  275 | ------ | Epi_R:  -0.99 | Running_R:  -3.07 | Epsilon:  0.5\n",
      "Epi:  276 | ------ | Epi_R:  -0.62 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  277 | ------ | Epi_R:  -3.33 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  278 | ------ | Epi_R:  -1.01 | Running_R:  -3.03 | Epsilon:  0.5\n",
      "Epi:  279 | ------ | Epi_R:  -4.49 | Running_R:  -3.04 | Epsilon:  0.5\n",
      "Epi:  280 | ------ | Epi_R:  -3.98 | Running_R:  -3.05 | Epsilon:  0.5\n",
      "Epi:  281 | ------ | Epi_R:  -7.5 | Running_R:  -3.1 | Epsilon:  0.5\n",
      "Epi:  282 | ------ | Epi_R:  -4.55 | Running_R:  -3.11 | Epsilon:  0.5\n",
      "Epi:  283 | ------ | Epi_R:  -3.75 | Running_R:  -3.12 | Epsilon:  0.5\n",
      "Epi:  284 | ------ | Epi_R:  -4.24 | Running_R:  -3.13 | Epsilon:  0.5\n",
      "Epi:  285 | ------ | Epi_R:  -2.6 | Running_R:  -3.12 | Epsilon:  0.5\n",
      "Epi:  286 | ------ | Epi_R:  -4.44 | Running_R:  -3.14 | Epsilon:  0.5\n",
      "Epi:  287 | ------ | Epi_R:  -4.16 | Running_R:  -3.15 | Epsilon:  0.5\n",
      "Epi:  288 | ------ | Epi_R:  -5.02 | Running_R:  -3.17 | Epsilon:  0.5\n",
      "Epi:  289 | ------ | Epi_R:  -4.29 | Running_R:  -3.18 | Epsilon:  0.5\n",
      "Epi:  290 | ------ | Epi_R:  -7.26 | Running_R:  -3.22 | Epsilon:  0.5\n",
      "Epi:  291 | ------ | Epi_R:  -1.69 | Running_R:  -3.2 | Epsilon:  0.5\n",
      "Epi:  292 | ------ | Epi_R:  -1.79 | Running_R:  -3.19 | Epsilon:  0.5\n",
      "Epi:  293 | ------ | Epi_R:  -3.41 | Running_R:  -3.19 | Epsilon:  0.5\n",
      "Epi:  294 | ------ | Epi_R:  -2.91 | Running_R:  -3.19 | Epsilon:  0.5\n",
      "Epi:  295 | ------ | Epi_R:  -3.95 | Running_R:  -3.2 | Epsilon:  0.5\n",
      "Epi:  296 | ------ | Epi_R:  -5.06 | Running_R:  -3.22 | Epsilon:  0.5\n",
      "Epi:  297 | ------ | Epi_R:  -5.03 | Running_R:  -3.23 | Epsilon:  0.5\n",
      "Epi:  298 | ------ | Epi_R:  -2.24 | Running_R:  -3.22 | Epsilon:  0.5\n",
      "Epi:  299 | ------ | Epi_R:  -3.89 | Running_R:  -3.23 | Epsilon:  0.5\n",
      "Epi:  300 | ------ | Epi_R:  -3.64 | Running_R:  -3.23 | Epsilon:  0.5\n",
      "Epi:  301 | ------ | Epi_R:  -11.44 | Running_R:  -3.32 | Epsilon:  0.5\n",
      "Epi:  302 | ------ | Epi_R:  -5.88 | Running_R:  -3.34 | Epsilon:  0.5\n",
      "Epi:  303 | ------ | Epi_R:  -5.45 | Running_R:  -3.36 | Epsilon:  0.5\n",
      "Epi:  304 | ------ | Epi_R:  -4.49 | Running_R:  -3.37 | Epsilon:  0.5\n",
      "Epi:  305 | ------ | Epi_R:  -4.78 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  306 | ------ | Epi_R:  -1.68 | Running_R:  -3.37 | Epsilon:  0.5\n",
      "Epi:  307 | ------ | Epi_R:  -5.37 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  308 | ------ | Epi_R:  -2.57 | Running_R:  -3.38 | Epsilon:  0.5\n",
      "Epi:  309 | ------ | Epi_R:  -3.6 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  310 | ------ | Epi_R:  -5.79 | Running_R:  -3.41 | Epsilon:  0.5\n",
      "Epi:  311 | ------ | Epi_R:  -2.66 | Running_R:  -3.4 | Epsilon:  0.5\n",
      "Epi:  312 | ------ | Epi_R:  -3.7 | Running_R:  -3.4 | Epsilon:  0.5\n",
      "Epi:  313 | ------ | Epi_R:  -5.27 | Running_R:  -3.42 | Epsilon:  0.5\n",
      "Epi:  314 | ------ | Epi_R:  -6.94 | Running_R:  -3.46 | Epsilon:  0.5\n",
      "Epi:  315 | ------ | Epi_R:  -2.65 | Running_R:  -3.45 | Epsilon:  0.5\n",
      "Epi:  316 | ------ | Epi_R:  -2.78 | Running_R:  -3.44 | Epsilon:  0.5\n",
      "Epi:  317 | ------ | Epi_R:  -4.01 | Running_R:  -3.45 | Epsilon:  0.5\n",
      "Epi:  318 | ------ | Epi_R:  -3.82 | Running_R:  -3.45 | Epsilon:  0.5\n",
      "Epi:  319 | ------ | Epi_R:  -5.28 | Running_R:  -3.47 | Epsilon:  0.5\n",
      "Epi:  320 | ------ | Epi_R:  -3.06 | Running_R:  -3.47 | Epsilon:  0.5\n",
      "Epi:  321 | ------ | Epi_R:  -1.77 | Running_R:  -3.45 | Epsilon:  0.5\n",
      "Epi:  322 | ------ | Epi_R:  -3.6 | Running_R:  -3.45 | Epsilon:  0.5\n",
      "Epi:  323 | ------ | Epi_R:  -5.35 | Running_R:  -3.47 | Epsilon:  0.5\n",
      "Epi:  324 | ------ | Epi_R:  -4.09 | Running_R:  -3.48 | Epsilon:  0.5\n",
      "Epi:  325 | ------ | Epi_R:  -2.81 | Running_R:  -3.47 | Epsilon:  0.5\n",
      "Epi:  326 | ------ | Epi_R:  -3.14 | Running_R:  -3.47 | Epsilon:  0.5\n",
      "Epi:  327 | ------ | Epi_R:  -2.77 | Running_R:  -3.46 | Epsilon:  0.5\n",
      "Epi:  328 | ------ | Epi_R:  -8.99 | Running_R:  -3.52 | Epsilon:  0.5\n",
      "Epi:  329 | ------ | Epi_R:  -3.13 | Running_R:  -3.51 | Epsilon:  0.5\n",
      "Epi:  330 | ------ | Epi_R:  -5.86 | Running_R:  -3.53 | Epsilon:  0.5\n",
      "Epi:  331 | ------ | Epi_R:  -3.9 | Running_R:  -3.54 | Epsilon:  0.5\n",
      "Epi:  332 | ------ | Epi_R:  -1.98 | Running_R:  -3.52 | Epsilon:  0.5\n",
      "Epi:  333 | ------ | Epi_R:  -1.03 | Running_R:  -3.5 | Epsilon:  0.5\n",
      "Epi:  334 | ------ | Epi_R:  0.2 | Running_R:  -3.46 | Epsilon:  0.5\n",
      "Epi:  335 | ------ | Epi_R:  -1.34 | Running_R:  -3.44 | Epsilon:  0.5\n",
      "Epi:  336 | ------ | Epi_R:  -2.65 | Running_R:  -3.43 | Epsilon:  0.5\n",
      "Epi:  337 | ------ | Epi_R:  -1.72 | Running_R:  -3.42 | Epsilon:  0.5\n",
      "Epi:  338 | ------ | Epi_R:  -2.25 | Running_R:  -3.4 | Epsilon:  0.5\n",
      "Epi:  339 | ------ | Epi_R:  -2.76 | Running_R:  -3.4 | Epsilon:  0.5\n",
      "Epi:  340 | ------ | Epi_R:  -4.24 | Running_R:  -3.41 | Epsilon:  0.5\n",
      "Epi:  341 | ------ | Epi_R:  -2.35 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  342 | ------ | Epi_R:  -1.79 | Running_R:  -3.38 | Epsilon:  0.5\n",
      "Epi:  343 | ------ | Epi_R:  -3.44 | Running_R:  -3.38 | Epsilon:  0.5\n",
      "Epi:  344 | ------ | Epi_R:  -3.25 | Running_R:  -3.38 | Epsilon:  0.5\n",
      "Epi:  345 | ------ | Epi_R:  -0.21 | Running_R:  -3.35 | Epsilon:  0.5\n",
      "Epi:  346 | ------ | Epi_R:  -2.46 | Running_R:  -3.34 | Epsilon:  0.5\n",
      "Epi:  347 | ------ | Epi_R:  -2.6 | Running_R:  -3.33 | Epsilon:  0.5\n",
      "Epi:  348 | ------ | Epi_R:  -2.81 | Running_R:  -3.32 | Epsilon:  0.5\n",
      "Epi:  349 | ------ | Epi_R:  -3.42 | Running_R:  -3.33 | Epsilon:  0.5\n",
      "Epi:  350 | ------ | Epi_R:  -4.45 | Running_R:  -3.34 | Epsilon:  0.5\n",
      "Epi:  351 | ------ | Epi_R:  -3.89 | Running_R:  -3.34 | Epsilon:  0.5\n",
      "Epi:  352 | ------ | Epi_R:  -3.57 | Running_R:  -3.34 | Epsilon:  0.5\n",
      "Epi:  353 | ------ | Epi_R:  -2.46 | Running_R:  -3.34 | Epsilon:  0.5\n",
      "Epi:  354 | ------ | Epi_R:  -3.05 | Running_R:  -3.33 | Epsilon:  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epi:  355 | ------ | Epi_R:  -5.52 | Running_R:  -3.36 | Epsilon:  0.5\n",
      "Epi:  356 | ------ | Epi_R:  -6.33 | Running_R:  -3.38 | Epsilon:  0.5\n",
      "Epi:  357 | ------ | Epi_R:  -2.18 | Running_R:  -3.37 | Epsilon:  0.5\n",
      "Epi:  358 | ------ | Epi_R:  -3.74 | Running_R:  -3.38 | Epsilon:  0.5\n",
      "Epi:  359 | ------ | Epi_R:  -4.12 | Running_R:  -3.38 | Epsilon:  0.5\n",
      "Epi:  360 | ------ | Epi_R:  -4.52 | Running_R:  -3.4 | Epsilon:  0.5\n",
      "Epi:  361 | ------ | Epi_R:  -4.58 | Running_R:  -3.41 | Epsilon:  0.5\n",
      "Epi:  362 | ------ | Epi_R:  -0.88 | Running_R:  -3.38 | Epsilon:  0.5\n",
      "Epi:  363 | ------ | Epi_R:  -2.36 | Running_R:  -3.37 | Epsilon:  0.5\n",
      "Epi:  364 | ------ | Epi_R:  -1.87 | Running_R:  -3.36 | Epsilon:  0.5\n",
      "Epi:  365 | ------ | Epi_R:  -3.61 | Running_R:  -3.36 | Epsilon:  0.5\n",
      "Epi:  366 | ------ | Epi_R:  -6.34 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  367 | ------ | Epi_R:  -3.95 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  368 | ------ | Epi_R:  -4.9 | Running_R:  -3.41 | Epsilon:  0.5\n",
      "Epi:  369 | ------ | Epi_R:  -3.26 | Running_R:  -3.41 | Epsilon:  0.5\n",
      "Epi:  370 | ------ | Epi_R:  -7.9 | Running_R:  -3.45 | Epsilon:  0.5\n",
      "Epi:  371 | ------ | Epi_R:  -1.35 | Running_R:  -3.43 | Epsilon:  0.5\n",
      "Epi:  372 | ------ | Epi_R:  -4.77 | Running_R:  -3.45 | Epsilon:  0.5\n",
      "Epi:  373 | ------ | Epi_R:  -2.37 | Running_R:  -3.43 | Epsilon:  0.5\n",
      "Epi:  374 | ------ | Epi_R:  -1.75 | Running_R:  -3.42 | Epsilon:  0.5\n",
      "Epi:  375 | ------ | Epi_R:  -1.33 | Running_R:  -3.4 | Epsilon:  0.5\n",
      "Epi:  376 | ------ | Epi_R:  -2.86 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  377 | ------ | Epi_R:  -4.3 | Running_R:  -3.4 | Epsilon:  0.5\n",
      "Epi:  378 | ------ | Epi_R:  -2.13 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  379 | ------ | Epi_R:  -4.06 | Running_R:  -3.39 | Epsilon:  0.5\n",
      "Epi:  380 | ------ | Epi_R:  -7.88 | Running_R:  -3.44 | Epsilon:  0.5\n",
      "Epi:  381 | ------ | Epi_R:  -1.9 | Running_R:  -3.42 | Epsilon:  0.5\n",
      "Epi:  382 | ------ | Epi_R:  -3.88 | Running_R:  -3.43 | Epsilon:  0.5\n",
      "Epi:  383 | ------ | Epi_R:  -3.44 | Running_R:  -3.43 | Epsilon:  0.5\n",
      "Epi:  384 | ------ | Epi_R:  -3.05 | Running_R:  -3.43 | Epsilon:  0.5\n",
      "Epi:  385 | ------ | Epi_R:  -1.35 | Running_R:  -3.4 | Epsilon:  0.5\n",
      "Epi:  386 | ------ | Epi_R:  -0.65 | Running_R:  -3.38 | Epsilon:  0.501\n",
      "Epi:  387 | ------ | Epi_R:  0.47 | Running_R:  -3.34 | Epsilon:  0.502\n",
      "Epi:  388 | ------ | Epi_R:  0.35 | Running_R:  -3.3 | Epsilon:  0.503\n",
      "Epi:  389 | ------ | Epi_R:  -1.77 | Running_R:  -3.29 | Epsilon:  0.504\n",
      "Epi:  390 | ------ | Epi_R:  -2.02 | Running_R:  -3.27 | Epsilon:  0.505\n",
      "Epi:  391 | ------ | Epi_R:  0.35 | Running_R:  -3.24 | Epsilon:  0.507\n",
      "Epi:  392 | ------ | Epi_R:  -2.39 | Running_R:  -3.23 | Epsilon:  0.51\n",
      "Epi:  393 | ------ | Epi_R:  -3.02 | Running_R:  -3.23 | Epsilon:  0.513\n",
      "Epi:  394 | ------ | Epi_R:  -0.06 | Running_R:  -3.2 | Epsilon:  0.523\n",
      "Epi:  395 | ------ | Epi_R:  0.44 | Running_R:  -3.16 | Epsilon:  0.525\n",
      "Epi:  396 | ------ | Epi_R:  0.06 | Running_R:  -3.13 | Epsilon:  0.526\n",
      "Epi:  397 | ------ | Epi_R:  -1.13 | Running_R:  -3.11 | Epsilon:  0.527\n",
      "Epi:  398 | ------ | Epi_R:  -1.22 | Running_R:  -3.09 | Epsilon:  0.53\n",
      "Epi:  399 | ------ | Epi_R:  0.42 | Running_R:  -3.05 | Epsilon:  0.531\n",
      "Epi:  400 | ------ | Epi_R:  0.1 | Running_R:  -3.02 | Epsilon:  0.532\n",
      "Epi:  401 | ------ | Epi_R:  -0.18 | Running_R:  -2.99 | Epsilon:  0.534\n",
      "Epi:  402 | ------ | Epi_R:  -0.64 | Running_R:  -2.97 | Epsilon:  0.535\n",
      "Epi:  403 | ------ | Epi_R:  -0.67 | Running_R:  -2.95 | Epsilon:  0.537\n",
      "Epi:  404 | ------ | Epi_R:  -0.28 | Running_R:  -2.92 | Epsilon:  0.538\n",
      "Epi:  405 | ------ | Epi_R:  0.17 | Running_R:  -2.89 | Epsilon:  0.541\n",
      "Epi:  406 | ------ | Epi_R:  0.07 | Running_R:  -2.86 | Epsilon:  0.543\n",
      "Epi:  407 | ------ | Epi_R:  0.15 | Running_R:  -2.83 | Epsilon:  0.544\n",
      "Epi:  408 | ------ | Epi_R:  -1.47 | Running_R:  -2.82 | Epsilon:  0.545\n",
      "Epi:  409 | ------ | Epi_R:  1.01 | Running_R:  -2.78 | Epsilon:  0.548\n",
      "Epi:  410 | ------ | Epi_R:  0.94 | Running_R:  -2.74 | Epsilon:  0.55\n",
      "Epi:  411 | ------ | Epi_R:  -1.29 | Running_R:  -2.73 | Epsilon:  0.553\n",
      "Epi:  412 | ------ | Epi_R:  0.34 | Running_R:  -2.69 | Epsilon:  0.556\n",
      "Epi:  413 | ------ | Epi_R:  0.79 | Running_R:  -2.66 | Epsilon:  0.559\n",
      "Epi:  414 | ------ | Epi_R:  -0.07 | Running_R:  -2.63 | Epsilon:  0.56\n",
      "Epi:  415 | ------ | Epi_R:  -0.32 | Running_R:  -2.61 | Epsilon:  0.561\n",
      "Epi:  416 | ------ | Epi_R:  0.02 | Running_R:  -2.58 | Epsilon:  0.563\n",
      "Epi:  417 | ------ | Epi_R:  0.75 | Running_R:  -2.55 | Epsilon:  0.565\n",
      "Epi:  418 | ------ | Epi_R:  0.77 | Running_R:  -2.52 | Epsilon:  0.567\n",
      "Epi:  419 | ------ | Epi_R:  0.19 | Running_R:  -2.49 | Epsilon:  0.568\n",
      "Epi:  420 | ------ | Epi_R:  0.13 | Running_R:  -2.46 | Epsilon:  0.571\n",
      "Epi:  421 | ------ | Epi_R:  0.5 | Running_R:  -2.43 | Epsilon:  0.573\n",
      "Epi:  422 | ------ | Epi_R:  -0.23 | Running_R:  -2.41 | Epsilon:  0.576\n",
      "Epi:  423 | ------ | Epi_R:  0.42 | Running_R:  -2.38 | Epsilon:  0.578\n",
      "Epi:  424 | ------ | Epi_R:  -1.6 | Running_R:  -2.38 | Epsilon:  0.579\n",
      "Epi:  425 | ------ | Epi_R:  -0.01 | Running_R:  -2.35 | Epsilon:  0.583\n",
      "Epi:  426 | ------ | Epi_R:  0.02 | Running_R:  -2.33 | Epsilon:  0.585\n",
      "Epi:  427 | ------ | Epi_R:  0.83 | Running_R:  -2.3 | Epsilon:  0.587\n",
      "Epi:  428 | ------ | Epi_R:  0.13 | Running_R:  -2.27 | Epsilon:  0.588\n",
      "Epi:  429 | ------ | Epi_R:  0.49 | Running_R:  -2.25 | Epsilon:  0.592\n",
      "Epi:  430 | ------ | Epi_R:  -0.03 | Running_R:  -2.22 | Epsilon:  0.594\n",
      "Epi:  431 | ------ | Epi_R:  0.35 | Running_R:  -2.2 | Epsilon:  0.596\n",
      "Epi:  432 | ------ | Epi_R:  -0.1 | Running_R:  -2.18 | Epsilon:  0.598\n",
      "Epi:  433 | ------ | Epi_R:  -0.96 | Running_R:  -2.16 | Epsilon:  0.603\n",
      "Epi:  434 | ------ | Epi_R:  -0.15 | Running_R:  -2.14 | Epsilon:  0.604\n",
      "Epi:  435 | Landed | Epi_R:  1.88 | Running_R:  -2.1 | Epsilon:  0.611\n",
      "Epi:  436 | ------ | Epi_R:  0.23 | Running_R:  -2.08 | Epsilon:  0.612\n",
      "Epi:  437 | ------ | Epi_R:  0.32 | Running_R:  -2.06 | Epsilon:  0.614\n",
      "Epi:  438 | ------ | Epi_R:  -0.16 | Running_R:  -2.04 | Epsilon:  0.617\n",
      "Epi:  439 | ------ | Epi_R:  0.34 | Running_R:  -2.01 | Epsilon:  0.627\n",
      "Epi:  440 | ------ | Epi_R:  -0.25 | Running_R:  -2.0 | Epsilon:  0.632\n",
      "Epi:  441 | ------ | Epi_R:  0.33 | Running_R:  -1.97 | Epsilon:  0.634\n",
      "Epi:  442 | ------ | Epi_R:  0.44 | Running_R:  -1.95 | Epsilon:  0.635\n",
      "Epi:  443 | ------ | Epi_R:  -0.33 | Running_R:  -1.93 | Epsilon:  0.638\n",
      "Epi:  444 | ------ | Epi_R:  -0.59 | Running_R:  -1.92 | Epsilon:  0.643\n",
      "Epi:  445 | ------ | Epi_R:  -0.46 | Running_R:  -1.91 | Epsilon:  0.653\n",
      "Epi:  446 | ------ | Epi_R:  -1.62 | Running_R:  -1.9 | Epsilon:  0.661\n",
      "Epi:  447 | ------ | Epi_R:  1.24 | Running_R:  -1.87 | Epsilon:  0.662\n",
      "Epi:  448 | ------ | Epi_R:  -1.4 | Running_R:  -1.87 | Epsilon:  0.672\n",
      "Epi:  449 | ------ | Epi_R:  -0.98 | Running_R:  -1.86 | Epsilon:  0.68\n",
      "Epi:  450 | ------ | Epi_R:  0.61 | Running_R:  -1.83 | Epsilon:  0.682\n",
      "Epi:  451 | ------ | Epi_R:  0.33 | Running_R:  -1.81 | Epsilon:  0.684\n",
      "Epi:  452 | ------ | Epi_R:  -0.23 | Running_R:  -1.8 | Epsilon:  0.694\n",
      "Epi:  453 | ------ | Epi_R:  -0.69 | Running_R:  -1.78 | Epsilon:  0.704\n",
      "Epi:  454 | ------ | Epi_R:  -0.66 | Running_R:  -1.77 | Epsilon:  0.714\n",
      "Epi:  455 | ------ | Epi_R:  -0.15 | Running_R:  -1.76 | Epsilon:  0.724\n",
      "Epi:  456 | ------ | Epi_R:  -1.22 | Running_R:  -1.75 | Epsilon:  0.734\n",
      "Epi:  457 | ------ | Epi_R:  -0.49 | Running_R:  -1.74 | Epsilon:  0.744\n",
      "Epi:  458 | ------ | Epi_R:  -1.02 | Running_R:  -1.73 | Epsilon:  0.754\n",
      "Epi:  459 | ------ | Epi_R:  -1.46 | Running_R:  -1.73 | Epsilon:  0.764\n",
      "Epi:  460 | ------ | Epi_R:  -0.36 | Running_R:  -1.72 | Epsilon:  0.774\n",
      "Epi:  461 | ------ | Epi_R:  -0.74 | Running_R:  -1.71 | Epsilon:  0.784\n",
      "Epi:  462 | ------ | Epi_R:  -1.33 | Running_R:  -1.7 | Epsilon:  0.794\n",
      "Epi:  463 | ------ | Epi_R:  -0.75 | Running_R:  -1.69 | Epsilon:  0.804\n",
      "Epi:  464 | ------ | Epi_R:  -0.63 | Running_R:  -1.68 | Epsilon:  0.814\n",
      "Epi:  465 | ------ | Epi_R:  -1.57 | Running_R:  -1.68 | Epsilon:  0.822\n",
      "Epi:  466 | ------ | Epi_R:  -1.13 | Running_R:  -1.67 | Epsilon:  0.829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b5934ba3c52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mMEMORY_CAPACITY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mland\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'| Landed'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mr_scale\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'| ------'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-68e3936799ec>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             feed_dict={self.s_: batch_memory[:, -self.n_features:],  # next observation\n\u001b[0;32m--> 267\u001b[0;31m                        self.s: batch_memory[:, -self.n_features:]})  # next observation\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mq_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    938\u001b[0m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m             raise ValueError(\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \"\"\"\n\u001b[0;32m--> 734\u001b[0;31m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rl/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    437\u001b[0m       raise TypeError(\"A string has ambiguous TensorShape, please wrap in a \"\n\u001b[1;32m    438\u001b[0m                        \"list or convert to an int: %s\" % dims)\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShapeProto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munknown_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "# env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "N_A = env.action_space.n\n",
    "N_S = env.observation_space.shape[0]\n",
    "MEMORY_CAPACITY = 50000\n",
    "TARGET_REP_ITER = 2000\n",
    "MAX_EPISODES = 900\n",
    "E_GREEDY = 0.95\n",
    "E_INCREMENT = 0.00001\n",
    "GAMMA = 0.99\n",
    "LR = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN = [400, 400]\n",
    "RENDER = True\n",
    "\n",
    "RL = DuelingDQNPrioritizedReplay(\n",
    "    n_actions=N_A, n_features=N_S, learning_rate=LR, e_greedy=E_GREEDY, reward_decay=GAMMA,\n",
    "    hidden=HIDDEN, batch_size=BATCH_SIZE, replace_target_iter=TARGET_REP_ITER,\n",
    "    memory_size=MEMORY_CAPACITY, e_greedy_increment=E_INCREMENT,)\n",
    "\n",
    "\n",
    "total_steps = 0\n",
    "running_r = 0\n",
    "r_scale = 100\n",
    "for i_episode in range(MAX_EPISODES):\n",
    "    s = env.reset()  # (coord_x, coord_y, vel_x, vel_y, angle, angular_vel, l_leg_on_ground, r_leg_on_ground)\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        if total_steps > MEMORY_CAPACITY: env.render()\n",
    "        a = RL.choose_action(s)\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        if r == -100: r = -30\n",
    "        r /= r_scale\n",
    "\n",
    "        ep_r += r\n",
    "        RL.store_transition(s, a, r, s_)\n",
    "        if total_steps > MEMORY_CAPACITY:\n",
    "            RL.learn()\n",
    "        if done:\n",
    "            land = '| Landed' if r == 100/r_scale else '| ------'\n",
    "            running_r = 0.99 * running_r + 0.01 * ep_r\n",
    "            print('Epi: ', i_episode,\n",
    "                  land,\n",
    "                  '| Epi_R: ', round(ep_r, 2),\n",
    "                  '| Running_R: ', round(running_r, 2),\n",
    "                  '| Epsilon: ', round(RL.epsilon, 3))\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "        total_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
